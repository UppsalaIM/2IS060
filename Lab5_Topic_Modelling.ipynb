{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 / Topic Modelling with LDA\n",
    "\n",
    "<sup>This lab is based on https://towardsdatascience.com/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc</sup>\n",
    "\n",
    "Before we get started, we need to load the relevant Python modules that us used in the code later by running the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import somialabs.lab5\n",
    "%matplotlib inline\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this lab:\n",
    "1. Follow the instructions running the code when asked.\n",
    "2. Discuss each question in your group.\n",
    "3. Keep notes for your answers to the questions in a separate MS Word document (you can use [this template](Lab5_answers_template.docx)).\n",
    "4. When completed, submit your answers to Studentportalen under Lab 5: http://www.uu.se/goto/boxz243.\n",
    "\n",
    "In this lab session, we will try out some *topic modelling*. \n",
    "\n",
    "A recurring subject in natural language processing and text analytics is to understand large corpus of texts through topics extraction. Whether you analyze users' online reviews, products' descriptions, or text entered in search bars, understanding key topics will always come in handy. This also applies to social media data, such as user posts on social networks, emails, and blogs.\n",
    "\n",
    "In the last lab, we looked at *sentiment analysis*, which aims to determine the emotional tone behind words. By doing this we can classify documents as being positive or negative in tone. In topic modelling, we try and extract what documents are talking about by using statistics and computation.\n",
    "\n",
    "In this lab, you will see how we can extract possible topics from articles posted on the BBC News website.\n",
    "\n",
    "<img src=\"images/lda.png\">\n",
    "\n",
    "Before going into the LDA method, let me remind you that not reinventing the wheel and going for the quick solution is usually the best start. Several providers have great API for topic extraction (and it is free up to a certain number of calls): [Google](https://cloud.google.com/natural-language/), [Microsoft](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking), [MeaningCloud](https://www.meaningcloud.com/developer/topics-extraction)... and all of the three and all work very well.\n",
    "\n",
    "However, if your data is highly specific, and no generic topic can represent it, then you will have to go for a more personalized approach. This lab focuses on one of these approaches: LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding LDA\n",
    "\n",
    "### Intuition\n",
    "LDA (Latent Dirichlet Allocation) is an unsupervised machine-learning technique that takes documents as input and finds topics as output. The model also says in what percentage each document talks about each topic.\n",
    "\n",
    "A topic is represented as a weighted list of words. An example of a topic is shown below:\n",
    "\n",
    "    flower * 0,2 | rose * 0,15 | plant * 0,09 | ...\n",
    "    \n",
    "<img src=\"images/lda_workflow.png\">\n",
    "\n",
    "There are 3 main parameters of the model:\n",
    "\n",
    "- the number of topics\n",
    "- the number of words per topic\n",
    "- the number of topics per document.\n",
    "\n",
    "In reality, the last two parameters are not exactly designed like this in the algorithm, but I prefer to stick to these simplified versions which are easier to understand.\n",
    "\n",
    "Run the code in the following cell to take a look at the BBC news document corpus that we will work with in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.view_bbc_corpus_head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Inspect a few of the documents listed from the BBC news corpus. Discuss in your group what words are important in determining topics by looking at the text of a document. How would you distinguish the important words from the non-important words that define a topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code cell to interact with the document corpus by filtering by language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.interact_bbc_corpus_by_lang()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** The corpus contains documents taken from more than one language. Using the drop-down box above, explore which languages of documents are contained in the corpus. Which languages are present? What challenges might this create when doing topic modelling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try generating topic models on the corpus of only English language documents. The underlying code uses a package called *gensim* that implements LDA for you already.\n",
    "\n",
    "Run the following code cell to do some preprocessing for topic modelling on articles from the BBC documents corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.preprocess_to_pos_tagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this has done is taken the original article text, split it into a list of sentences, then taken each sentence and split it into a list of words (at which point we have a list of lists of words), then it applies part-of-speech tagging (POS) to classify each word according some kind of word-type.\n",
    "\n",
    "Let's look closer at the POS tags allocated to each token in the first article. Run the following code cell to generate a table summarising the words and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.preprocess_to_pos_tagging_and_get_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the POS tag list for what each tag means (and some examples in English):\n",
    "```\n",
    "CC   coordinating conjunction\n",
    "CD   cardinal digit\n",
    "DT   determiner\n",
    "EX   existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW   foreign word\n",
    "IN   preposition/subordinating conjunction\n",
    "JJ   adjective\t'big'\n",
    "JJR  adjective, comparative\t'bigger'\n",
    "JJS  adjective, superlative\t'biggest'\n",
    "LS   list marker\t1)\n",
    "MD   modal\tcould, will\n",
    "NN   noun, singular 'desk'\n",
    "NNS  noun plural\t'desks'\n",
    "NNP  proper noun, singular\t'Harrison'\n",
    "NNPS proper noun, plural\t'Americans'\n",
    "PDT  predeterminer\t'all the kids'\n",
    "POS  possessive ending\tparent's\n",
    "PRP  personal pronoun\tI, he, she\n",
    "PRP$ possessive pronoun\tmy, his, hers\n",
    "RB   adverb\tvery, silently,\n",
    "RBR  adverb, comparative\tbetter\n",
    "RBS  adverb, superlative\tbest\n",
    "RP   particle\tgive up\n",
    "TO   to\tgo 'to' the store.\n",
    "UH   interjection\terrrrrrrrm\n",
    "VB   verb, base form\ttake\n",
    "VBD  verb, past tense\ttook\n",
    "VBG  verb, gerund/present participle\ttaking\n",
    "VBN  verb, past participle\ttaken\n",
    "VBP  verb, sing. present, non-3d\ttake\n",
    "VBZ  verb, 3rd person sing. present\ttakes\n",
    "WDT  wh-determiner\twhich\n",
    "WP   wh-pronoun\twho, what\n",
    "WP$  possessive wh-pronoun\twhose\n",
    "WRB  wh-abverb\twhere, when\n",
    "```\n",
    "\n",
    "**Question 3.** Do POS tags help identify different words' definition and context? Why might POS tagging be helpful for topic modelling? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.preprocess_to_lemmatization_and_get_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Compare the output above after lemmatization to the output after only POS tagging. How has lemmatization changed the text? How might this effect generated topics from topic modelling?\n",
    "\n",
    "Run the following code cell to interact with the lemmatization function. Try typing your own text to test the lemmatization behaviour. For example, try something like `jump jumping jumped`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.interact_lemmatization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code cell to generate topic models from the BBC articles corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.gensim_on_bbc_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Topics generated consist of a list of words and a corresponding weight (value) to indicate how important a word is in the topic model given. Take a look at the topics generated from the code above. Are they easy to interpret? In your group try and determine what each of the 20 topics listed might be about.\n",
    "\n",
    "Run the following code cell to generate topic models from the BBC articles corpus, and then use the interactive slider to explore some of the articles and the matched topics from the topic modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.interact_gensim_on_bbc_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Explore several articles and inspect the corresponding topics that the articles are matched to. How well do these article-topic matches correspond to your own group's interpretation of the topic models that were generated? What makes using computer-genreated topics difficult to use?\n",
    "\n",
    "Run the following code cell to view a heatmap visualization of the proportion of topics in the documents (Documents are rows, topic are columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "somialabs.lab5.visualize_topic_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**. What do you observe about the distribution of the number of documents that have been allocated to each topic?\n",
    "\n",
    "Run the following code cell to interact with a visualization of the the LDA topic model. This visualization allows you to compare topics on two reduced dimensions and observe the distribution of words in topics. Take a look at the most relevant terms for various topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somialabs.lab5.visualize_lda_model()  # be patient, this one takes a little longer to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8.** What do you observe about the top relevant terms for different topics? How might the topic model be improved using data preprocessing? \n",
    "\n",
    "**Question 9.** The inter-topic visualization tries to show how similar some topics are. Do the topics overlap a lot? If so, why might this be problematic in determining topics for different articles?\n",
    "\n",
    "**Question 10.** Do all of the topics generated cover all possible topics about the documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to successfully implement LDA\n",
    "LDA is a complex algorithm which is generally perceived as hard to fine-tune and interpret. Indeed, getting relevant results with LDA requires a strong knowledge of how it works. In this lab we did not go into the technical details of how it works, but only explored on a high level how to use something like LDA for topic modelling.\n",
    "\n",
    "### Data cleaning\n",
    "A common thing you will encounter with LDA is that words appear in multiple topics. One way to cope with this is to add these words to your stopwords list.\n",
    "\n",
    "Another thing is plural and singular forms. I would recommend lemmatizing — or stemming if you cannot lemmatize but having stems in your topics is not easily understandable.\n",
    "\n",
    "Removing words with digits in them will also clean the words in your topics. Keeping years (2006, 1981) can be relevant if you believe they are meaningful in your topics.\n",
    "\n",
    "Filtering words that appear in at least 3 (or more) documents is a good way to remove rare words that will not be relevant in topics.\n",
    "\n",
    "### Data preparation\n",
    "Include bi- and tri-grams to grasp more relevant information.\n",
    "\n",
    "Another classic preparation step is to use only nouns and verbs using POS tagging (POS: Part-Of-Speech).\n",
    "\n",
    "### Fine-tuning\n",
    "- Number of topics: try out several numbers of topics to understand which amount makes sense. You actually need to see the topics to know if your model makes sense or not. As for K-Means, LDA converges and the model makes sense at a mathematical level, but it does not mean it makes sense at a human level.\n",
    "- Cleaning your data: adding stop words that are too frequent in your topics and re-running your model is a common step. Keeping only nouns and verbs, removing templates from texts, testing different cleaning methods iteratively will improve your topics. Be prepared to spend some time here.\n",
    "- Alpha, Eta. We did not touch on the technical stuff in this lab. If implementing LDA using code, you can tweak alpha and eta to adjust your topics. Start with 'auto', and if the topics are not relevant, try other values. I recommend using low values of Alpha and Eta to have a small number of topics in each document and a small number of relevant words in each topic.\n",
    "- Increase the number of passes to have a better model. 3 or 4 is a good number, but you can go higher.\n",
    "\n",
    "### Assessing results\n",
    "- Are your topics interpretable?\n",
    "- Are your topics unique? (two different topics have different words)\n",
    "- Are your topics exhaustive? (are all your documents well represented by these topics?)\n",
    "\n",
    "If your model follows these 3 criteria, it looks like a good model :)\n",
    "\n",
    "## Main advantages of LDA\n",
    "### It’s fast\n",
    "\n",
    "Use the %time command in Jupyter to verify it. The model is usually fast to run. Of course, it depends on your data. Several factors can slow down the model:\n",
    "\n",
    "- Long documents\n",
    "- Large number of documents\n",
    "- Large vocabulary size (especially if you use n-grams with a large n)\n",
    "\n",
    "### It’s intuitive\n",
    "Modelling topics as weighted lists of words is a simple approximation yet a very intuitive approach if you need to interpret it. No embedding nor hidden dimensions, just bags of words with weights.\n",
    "\n",
    "### It can predict topics for new unseen documents\n",
    "\n",
    "Once the model has run, it is ready to allocate topics to any document. Of course, if your training dataset is in English and you want to predict the topics of a Chinese document it won’t work. But if the new documents have the same structure and should have more or less the same topics, it will work.\n",
    "\n",
    "## Main disadvantages of LDA\n",
    "### Lots of fine-tuning\n",
    "\n",
    "If LDA is fast to run, it will give you some trouble to get good results with it. That’s why knowing in advance how to fine-tune it will really help you.\n",
    "\n",
    "### It needs human interpretation\n",
    "\n",
    "Topics are found by a machine. A human needs to label them in order to present the results to non-experts people.\n",
    "\n",
    "### You cannot influence topics\n",
    "\n",
    "Knowing that some of your documents talk about a topic you know, and not finding it in the topics found by LDA will definitely be frustrating. And there’s no way to say to the model that some words should belong together. You have to sit and wait for the LDA to give you what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
